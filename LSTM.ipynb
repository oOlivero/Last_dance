{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ole\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ole\\Anaconda3\\lib\\site-packages\\pandas_datareader\\compat\\__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "#基于lstm量化交易系统\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from pandas_datareader import data as pdr\n",
    "import datetime\n",
    "import time\n",
    "import yfinance as yf\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Access Key': 'dqnh6tvdf3-384f7880-6d85af4e-d6ec5', 'Secret Key': '8ca950a5-3f6b29db-998ea629-08452'}\n"
     ]
    }
   ],
   "source": [
    "#配置火币API\n",
    "from HuobiDMService import HuobiDM\n",
    "from pprint import pprint\n",
    "\n",
    "# 输入火币合约交易的api地址\n",
    "URL = 'https://api.btcgateway.pro'\n",
    "with open(\"../api_key.json\",'r') as load_f:\n",
    "    api_key = json.load(load_f)\n",
    "    print(api_key)\n",
    "#  输入 access_key 和 secret_key :\n",
    "ACCESS_KEY = api_key['Access Key']\n",
    "SECRET_KEY = api_key['Secret Key']\n",
    "\n",
    "\n",
    "dm = HuobiDM(URL, ACCESS_KEY, SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#配置matplotlib画图的符号\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  #显示中文\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示坐标中的负号\n",
    " \n",
    "def get_stock_data(name,start,end):\n",
    "    try:\n",
    "        yf.pdr_override()\n",
    "    #实时股票数据\n",
    "        if type(start) is str:\n",
    "            start = datetime.datetime.strptime(start,'%Y/%m/%d')\n",
    "        if type(end) is str:\n",
    "            end = datetime.datetime.strptime(end,'%Y/%m/%d')\n",
    "        finance = pdr.get_data_yahoo(name,start,end)  \n",
    "        data = np.array(finance['Close']) #获取收盘价的数据\n",
    "        data = data[::1] #获取这列的所有数据\n",
    "        print('股票数据获取完成！！')\n",
    "        return data\n",
    "    except Exception:\n",
    "        print('股票数据获取失败！！')\n",
    "def get_coin_data(name):\n",
    "    try:\n",
    "        finance = pd.DataFrame(dm.get_contract_kline(symbol=name, period='5min', size=2000)['data'])\n",
    "        data = np.array(finance['close']) #获取收盘价的数据\n",
    "        data = data[::1] #获取这列的所有数据\n",
    "        print(data)\n",
    "        print('数据获取完成！！')\n",
    "        return data\n",
    "    except Exception:\n",
    "        print('数据获取失败！！')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "股票数据获取完成！！\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = get_stock_data('601318.SS','2013/11/22','2020/1/4')\n",
    "#以折线图展示导入的数据\n",
    "fig =plt.figure()\n",
    "#fig.add_subplot(1,2,1)\n",
    "#plt.plot(data)\n",
    "#plt.show()\n",
    "normalize_data=(data-np.mean(data))/np.std(data)  #对数据进行标准化 （数据 - 均值）/（方差）\n",
    "normalize_data=normalize_data[:,np.newaxis]       #增加数据的维度，使数据维度相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#———————————————————形成训练集—————————————————————\n",
    "#设置rnn网络的常量\n",
    "time_step=20     #时间步 ，rnn每迭代20次，就向前推进一步\n",
    "rnn_unit=10       # hidden layer units\n",
    "batch_size=60     # 每一批训练多少个样例\n",
    "input_size=1      # 输入层数维度\n",
    "output_size=1     # 输出层数维度\n",
    "lr=0.0006         # 学习率\n",
    "train_x,train_y=[],[]   #训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#———————————————————形成训练集—————————————————————\n",
    "for i in range(len(normalize_data)-time_step-1):\n",
    "    x=normalize_data[i:i+time_step]\n",
    "    y=normalize_data[i+1:i+time_step+1]\n",
    "    train_x.append(x.tolist())\n",
    "    train_y.append(y.tolist()) \n",
    "#print(len(normalize_data))\n",
    "#print(train_x)\n",
    "#print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#———————————————————定义神经网络变量—————————————————————\n",
    "\n",
    "X=tf.placeholder(tf.float32, [None,time_step,input_size])    #每批次输入网络的tensor\n",
    "Y=tf.placeholder(tf.float32, [None,time_step,output_size])   # 每批次tensor对应的标签\n",
    "#输入层、输出层的权重和偏置\n",
    "weights={\n",
    "         'in':tf.Variable(tf.random_normal([input_size,rnn_unit])),\n",
    "         'out':tf.Variable(tf.random_normal([rnn_unit,1]))\n",
    "         }\n",
    "biases={\n",
    "        'in':tf.Variable(tf.constant(0.1,shape=[rnn_unit,])),\n",
    "        'out':tf.Variable(tf.constant(0.1,shape=[1,]))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#———————————————————定义lstm网络—————————————————————\n",
    "def lstm(batch):      #参数：输入网络批次数目\n",
    "    w_in=weights['in']\n",
    "    b_in=biases['in']\n",
    "    input=tf.reshape(X,[-1,input_size])  #需要将tensor转为2维进行计算，计算后的结果作为 隐藏层的输入\n",
    "    input_rnn=tf.matmul(input,w_in)+b_in\n",
    "    input_rnn=tf.reshape(input_rnn,[-1,time_step,rnn_unit])   #将tensor转为3维，作为 lstm cell的输入\n",
    "    cell=tf.nn.rnn_cell.BasicLSTMCell(rnn_unit)\n",
    "    init_state=cell.zero_state(batch,dtype=tf.float32)\n",
    "    output_rnn,final_states=tf.nn.dynamic_rnn(cell, input_rnn,initial_state=init_state, dtype=tf.float32)\n",
    "    output=tf.reshape(output_rnn,[-1,rnn_unit])  #作为输出层的输入\n",
    "    w_out=weights['out']\n",
    "    b_out=biases['out']\n",
    "    pred=tf.matmul(output,w_out)+b_out\n",
    "    return pred,final_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#———————————————————对模型进行训练—————————————————————\n",
    "def train_lstm():\n",
    "    global batch_size\n",
    "    with tf.variable_scope(\"lstm\"):\n",
    "        pred,_=lstm(batch_size)\n",
    "    #定义损失函数\n",
    "    loss=tf.reduce_mean(tf.square(tf.reshape(pred,[-1])-tf.reshape(Y, [-1])))\n",
    "    train_op=tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "    saver=tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        loss_list = [] #收集损失值\n",
    "        for i in range(201): #模型训练的次数，We can increase the number of iterations to gain better result.\n",
    "            step=0\n",
    "            start=0\n",
    "            end=start+batch_size\n",
    "            while(end<len(train_x)):\n",
    "                _,loss_=sess.run([train_op,loss],feed_dict={X:train_x[start:end],Y:train_y[start:end]})\n",
    "                start+=batch_size\n",
    "                end=start+batch_size\n",
    "                #每训练10次保存一次参数\n",
    "                if step%10==0:\n",
    "                    print(\"Number of iterations:\",i,\" loss:\",loss_) #输出训练次数，输出损失值\n",
    "                    print(\"model_save\",saver.save(sess,'./model_save1/modle.ckpt')) #第二个参数是保存的地址，可以修改为自己本地的保存地址\n",
    "                    #I run the code in windows 10,so use  'model_save1\\\\modle.ckpt'\n",
    "                    #if you run it in Linux,please use  'model_save1/modle.ckpt'\n",
    "                    loss_list.append(loss_)\n",
    "                step+=1\n",
    "        print(\"The train has finished\")\n",
    "        print(len(loss_list))\n",
    "        sns.set_style(style = 'whitegrid') #详细参数看seaborn的API  http://seaborn.pydata.org/api.html\n",
    "        #设置字体\n",
    "        sns.set_context(context = 'poster',font_scale = 1)\n",
    "        plt.figure(figsize = (20,20)) #图像大小为20*20英寸\n",
    "        plt.plot(np.arange(0,len(loss_list)),loss_list,'+-',color = 'g')\n",
    "        plt.title('Loss trend')\n",
    "        plt.ylabel('Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ———————————————————预测模型—————————————————————\n",
    "def prediction():\n",
    "    with tf.variable_scope(\"lstm\",reuse=True):\n",
    "        pred,_=lstm(1)    #预测时只输入[1,time_step,input_size]的测试数据\n",
    "    saver=tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        #参数恢复\n",
    "        saver.restore(sess, './model_save1/modle.ckpt') #第二个参数是保存的地址，可以修改为自己本地的保存地址\n",
    "        #I run the code in windows 10,so use  'model_save1\\\\modle.ckpt'\n",
    "        #if you run it in Linux,please use  'model_save1/modle.ckpt'\n",
    "        \n",
    "        #取训练集最后一行为测试样本。shape = [1,time_step,input_size]\n",
    "        prev_seq=train_x[-1]\n",
    "        predict=[]\n",
    "        #得到之后的100个预测结果\n",
    "        for i in range(100):  #预测100个数值\n",
    "            next_seq=sess.run(pred,feed_dict={X:[prev_seq]})\n",
    "            predict.append(next_seq[-1])\n",
    "            #每次得到最后一个时间步的预测结果，与之前的数据加在一起，形成新的测试数据\n",
    "            prev_seq=np.vstack((prev_seq[1:],next_seq[-1]))\n",
    "        #以折线图展示结果\n",
    "        plt.figure(figsize = (20,20)) #图像大小为8*8英寸\n",
    "        #设置背景风格\n",
    "        sns.set_style(style = 'whitegrid') #详细参数看seaborn的API  http://seaborn.pydata.org/api.html\n",
    "        #设置字体\n",
    "        sns.set_context(context = 'poster',font_scale = 1)\n",
    "        plt.title('Prediction')\n",
    "        predict = np.squeeze(predict)#减少数据维度\n",
    "        predict = predict*np.std(data)+np.mean(data)#恢复先前价格\n",
    "        plt.plot(list(range(len(data))), data, color='b',label = 'raw data') #这是原来股票的价格走势，用蓝色曲线表示\n",
    "        plt.plot(list(range(len(data), len(data) + len(predict))), predict, color='r',label = 'predict trend') #预测未来的价格走势用红色表示\n",
    "        plt.legend(loc = 'best')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Price')\n",
    "        plt.show()\n",
    "#fig.add_subplot(1,2,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#———————————————————得到测试数据—————————————————————\n",
    "def get_stock_test_data(start,end,n):   \n",
    "    data = get_stock_data('601318.SS',start,end)\n",
    "    normalize_data=(data-np.mean(data))/np.std(data)  #对数据进行标准化 （数据 - 均值）/（方差）\n",
    "    normalize_data=normalize_data[:,np.newaxis]       #增加数据的维度，使数据维度相同\n",
    "    for i in range(len(normalize_data)-time_step-1):\n",
    "        x=normalize_data[i:i+time_step]\n",
    "        y=normalize_data[i+1:i+time_step+1]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist()) \n",
    "    with tf.variable_scope('lstm',reuse=True):\n",
    "        pred,last= lstm(1)\n",
    "    saver=tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        #参数恢复\n",
    "        saver.restore(sess, './model_save1/modle.ckpt') #第二个参数是保存的地址，可以修改为自己本地的保存地址\n",
    "        #I run the code in windows 10,so use  'model_save1\\\\modle.ckpt'\n",
    "        #if you run it in Linux,please use  'model_save1/modle.ckpt'\n",
    "        \n",
    "        #取训练集最后一行为测试样本。shape = [1,time_step,input_size]\n",
    "        prev_seq=train_x[-1]\n",
    "        predict=[]\n",
    "        #得到之后的100个预测结果\n",
    "        for i in range(n):  #预测100个数值\n",
    "            next_seq=sess.run(pred,feed_dict={X:[prev_seq]})\n",
    "            predict.append(next_seq[-1])\n",
    "            #每次得到最后一个时间步的预测结果，与之前的数据加在一起，形成新的测试数据\n",
    "            prev_seq=np.vstack((prev_seq[1:],next_seq[-1]))\n",
    "        predict = np.squeeze(predict)#减少数据维度\n",
    "        predict = predict*np.std(data)+np.mean(data)#恢复先前价格\n",
    "        return predict;\n",
    "        \n",
    "#fig.add_subplot(1,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-ddceb3fac83c>:8: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-ddceb3fac83c>:10: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\ole\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ole\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Number of iterations: 0  loss: 3.9998426\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 0  loss: 1.0714946\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 0  loss: 0.72912467\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 1  loss: 2.7736783\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 1  loss: 0.71233827\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 1  loss: 0.49761733\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 2  loss: 2.1023536\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 2  loss: 0.47395304\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 2  loss: 0.31809083\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 3  loss: 1.611393\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 3  loss: 0.30364487\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 3  loss: 0.18375768\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 4  loss: 1.2528834\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 4  loss: 0.18639451\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 4  loss: 0.09084291\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 5  loss: 0.9855452\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 5  loss: 0.10969958\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 5  loss: 0.040124312\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 6  loss: 0.77517897\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 6  loss: 0.06370262\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 6  loss: 0.029766649\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 7  loss: 0.6030753\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 7  loss: 0.040780157\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 7  loss: 0.047598604\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 8  loss: 0.46473974\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 8  loss: 0.03346276\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 8  loss: 0.07351366\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 9  loss: 0.36261395\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 9  loss: 0.034014676\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 9  loss: 0.09101545\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 10  loss: 0.29457858\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 10  loss: 0.036610175\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 10  loss: 0.09561901\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 11  loss: 0.25103045\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 11  loss: 0.038512237\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 11  loss: 0.09078007\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 12  loss: 0.222173\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 12  loss: 0.039076813\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 12  loss: 0.0812153\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 13  loss: 0.20159142\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 13  loss: 0.038478766\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 13  loss: 0.07022643\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 14  loss: 0.18569298\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 14  loss: 0.037063308\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 14  loss: 0.059608866\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 15  loss: 0.17255631\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 15  loss: 0.035142243\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 15  loss: 0.050185505\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 16  loss: 0.16116498\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 16  loss: 0.032962773\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 16  loss: 0.042246982\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 17  loss: 0.15098515\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 17  loss: 0.030719014\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 17  loss: 0.035809796\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 18  loss: 0.14173159\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 18  loss: 0.028559418\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 18  loss: 0.030746715\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 19  loss: 0.13323797\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 19  loss: 0.026581762\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 19  loss: 0.026853962\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 20  loss: 0.12539035\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 20  loss: 0.02482825\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 20  loss: 0.023895793\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 21  loss: 0.118111245\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 21  loss: 0.023293445\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 21  loss: 0.021642212\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 22  loss: 0.11135884\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 22  loss: 0.021944638\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 22  loss: 0.019897193\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 23  loss: 0.105114825\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 23  loss: 0.02074307\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 23  loss: 0.018510375\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 24  loss: 0.099363245\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 24  loss: 0.019656789\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 24  loss: 0.017374964\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 25  loss: 0.09407742\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 25  loss: 0.018664148\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 25  loss: 0.01641832\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 26  loss: 0.0892203\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 26  loss: 0.017751984\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 26  loss: 0.015591552\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 27  loss: 0.08475088\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 27  loss: 0.01691218\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 27  loss: 0.014861458\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 28  loss: 0.08063023\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 28  loss: 0.016139107\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 28  loss: 0.014205019\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 29  loss: 0.07682345\n",
      "model_save ./model_save1/modle.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 29  loss: 0.015428282\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 29  loss: 0.013605984\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 30  loss: 0.07330104\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 30  loss: 0.0147756385\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 30  loss: 0.013052676\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 31  loss: 0.07003729\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 31  loss: 0.014177379\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 31  loss: 0.012536621\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 32  loss: 0.06700986\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 32  loss: 0.01362988\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 32  loss: 0.01205166\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 33  loss: 0.064199015\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 33  loss: 0.013129579\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 33  loss: 0.011593273\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 34  loss: 0.06158713\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 34  loss: 0.012673022\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 34  loss: 0.011158203\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 35  loss: 0.05915811\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 35  loss: 0.012256787\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 35  loss: 0.010744078\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 36  loss: 0.05689739\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 36  loss: 0.011877514\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 36  loss: 0.010349227\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 37  loss: 0.054791532\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 37  loss: 0.011531915\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 37  loss: 0.0099724755\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 38  loss: 0.052828368\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 38  loss: 0.011216805\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 38  loss: 0.009613024\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 39  loss: 0.050996467\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 39  loss: 0.010929089\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 39  loss: 0.009270364\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 40  loss: 0.049285393\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 40  loss: 0.010665819\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 40  loss: 0.00894418\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 41  loss: 0.047685467\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 41  loss: 0.0104241865\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 41  loss: 0.008634213\n",
      "model_save ./model_save1/modle.ckpt\n",
      "Number of iterations: 42  loss: 0.046187773\n",
      "model_save ./model_save1/modle.ckpt\n"
     ]
    }
   ],
   "source": [
    "train_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def if_predict_true(start,end,n):   \n",
    "    data = get_stock_data('601318.SS',start,end)\n",
    "    real = data[-n:]\n",
    "    data = data[0:-n]\n",
    "    normalize_data=(data-np.mean(data))/np.std(data)  #对数据进行标准化 （数据 - 均值）/（方差）\n",
    "    normalize_data=normalize_data[:,np.newaxis]       #增加数据的维度，使数据维度相同\n",
    "    for i in range(len(normalize_data)-time_step-1):\n",
    "        x=normalize_data[i:i+time_step]\n",
    "        y=normalize_data[i+1:i+time_step+1]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist()) \n",
    "    with tf.variable_scope('lstm',reuse=True):\n",
    "        pred,last= lstm(1)\n",
    "    saver=tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        #参数恢复\n",
    "        saver.restore(sess, './model_save1/modle.ckpt') #第二个参数是保存的地址，可以修改为自己本地的保存地址\n",
    "        #I run the code in windows 10,so use  'model_save1\\\\modle.ckpt'\n",
    "        #if you run it in Linux,please use  'model_save1/modle.ckpt'\n",
    "\n",
    "        #取训练集最后一行为测试样本。shape = [1,time_step,input_size]\n",
    "        prev_seq=train_x[-1]\n",
    "        predict=[]\n",
    "        #得到之后的100个预测结果\n",
    "        for i in range(n):  #预测100个数值\n",
    "            next_seq=sess.run(pred,feed_dict={X:[prev_seq]})\n",
    "            predict.append(next_seq[-1])\n",
    "            #每次得到最后一个时间步的预测结果，与之前的数据加在一起，形成新的测试数据\n",
    "            prev_seq=np.vstack((prev_seq[1:],next_seq[-1]))\n",
    "        predict = np.squeeze(predict)#减少数据维度\n",
    "        predict = predict*np.std(data)+np.mean(data)#恢复先前价格\n",
    "        if (real[n-1]>data[-1] and predict[n-1]>data[-1]) or (real[n-1]<=data[-1] and predict[n-1]<=data[-1]):\n",
    "            return 1\n",
    "        else:return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if_predict_true(\"2014/2/3\",\"2016/3/3\",8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def add_a_day(t):\n",
    "    t = int(time.mktime(t.timetuple()))\n",
    "    t += 86400\n",
    "    t = datetime.datetime.fromtimestamp(t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "股票数据获取完成！！\n"
     ]
    }
   ],
   "source": [
    "sum = 0;\n",
    "start = \"2016/2/2\"\n",
    "end = \"2020/3/29\"\n",
    "test_data = get_stock_data('601318.SS',start,end)\n",
    "half = int(len(test_data)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/504\n",
      "INFO:tensorflow:Restoring parameters from ./model_save1/modle.ckpt\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for times in range(1,half):\n",
    "    t_data = test_data[times:times+half]\n",
    "    print(str(times)+\"/\"+str(half))\n",
    "    real = t_data[-1:]\n",
    "    t_data = t_data[0:-1]\n",
    "    normalize_t_data=(t_data-np.mean(t_data))/np.std(t_data)  #对数据进行标准化 （数据 - 均值）/（方差）\n",
    "    normalize_t_data=normalize_t_data[:,np.newaxis]       #增加数据的维度，使数据维度相同\n",
    "    for i in range(len(normalize_t_data)-time_step-1):\n",
    "        x=normalize_t_data[i:i+time_step]\n",
    "        y=normalize_t_data[i+1:i+time_step+1]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist()) \n",
    "    with tf.variable_scope('lstm',reuse=True):\n",
    "        pred,last= lstm(1)\n",
    "    saver=tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        #参数恢复\n",
    "        saver.restore(sess, './model_save1/modle.ckpt') #第二个参数是保存的地址，可以修改为自己本地的保存地址\n",
    "        #I run the code in windows 10,so use  'model_save1\\\\modle.ckpt'\n",
    "        #if you run it in Linux,please use  'model_save1/modle.ckpt'\n",
    "\n",
    "        #取训练集最后一行为测试样本。shape = [1,time_step,input_size]\n",
    "        prev_seq=train_x[-1]\n",
    "        predict=[]\n",
    "        #得到之后的100个预测结果\n",
    "        for i in range(2):  #预测100个数值\n",
    "            next_seq=sess.run(pred,feed_dict={X:[prev_seq]})\n",
    "            predict.append(next_seq[-1])\n",
    "            #每次得到最后一个时间步的预测结果，与之前的数据加在一起，形成新的测试数据\n",
    "            prev_seq=np.vstack((prev_seq[1:],next_seq[-1]))\n",
    "        predict = np.squeeze(predict)#减少数据维度\n",
    "        predict = predict*np.std(t_data)+np.mean(t_data)#恢复先前价格\n",
    "        print(\"实际：\"+str(real[0]))\n",
    "        print(\"预测: \"+str(predict[0]))\n",
    "        print(\"起价: \"+str(t_data[-1]))           \n",
    "        if (real[0]>t_data[-1] and predict[0]>t_data[-1]) or (real[0]<=t_data[-1] and predict[0]<=t_data[-1]):\n",
    "            sum = sum+1\n",
    "    print(sum/times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#废弃代码段\n",
    "def if_predict_true(start,end,n):   \n",
    "    data = get_stock_data('601318.SS',start,end)\n",
    "    real = data[-n:]\n",
    "    data = data[0:-n]\n",
    "    normalize_data=(data-np.mean(data))/np.std(data)  #对数据进行标准化 （数据 - 均值）/（方差）\n",
    "    normalize_data=normalize_data[:,np.newaxis]       #增加数据的维度，使数据维度相同\n",
    "    for i in range(len(normalize_data)-time_step-1):\n",
    "        x=normalize_data[i:i+time_step]\n",
    "        y=normalize_data[i+1:i+time_step+1]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist()) \n",
    "    with tf.variable_scope('lstm',reuse=True):\n",
    "        pred,last= lstm(1)\n",
    "    saver=tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        #参数恢复\n",
    "        saver.restore(sess, './model_save1/modle.ckpt') #第二个参数是保存的地址，可以修改为自己本地的保存地址\n",
    "        #I run the code in windows 10,so use  'model_save1\\\\modle.ckpt'\n",
    "        #if you run it in Linux,please use  'model_save1/modle.ckpt'\n",
    "\n",
    "        #取训练集最后一行为测试样本。shape = [1,time_step,input_size]\n",
    "        prev_seq=train_x[-1]\n",
    "        predict=[]\n",
    "        #得到之后的100个预测结果\n",
    "        for i in range(n):  #预测100个数值\n",
    "            next_seq=sess.run(pred,feed_dict={X:[prev_seq]})\n",
    "            predict.append(next_seq[-1])\n",
    "            #每次得到最后一个时间步的预测结果，与之前的数据加在一起，形成新的测试数据\n",
    "            prev_seq=np.vstack((prev_seq[1:],next_seq[-1]))\n",
    "        predict = np.squeeze(predict)#减少数据维度\n",
    "        predict = predict*np.std(data)+np.mean(data)#恢复先前价格\n",
    "        if (real[n-1]>data[-1] and predict[n-1]>data[-1]) or (real[n-1]<=data[-1] and predict[n-1]<=data[-1]):\n",
    "            return 1\n",
    "        else:return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#———————————————————得到测试数据—————————————————————\n",
    "def get_stock_test_data(start,end,n):   \n",
    "    data = get_stock_data('601318.SS',start,end)\n",
    "    normalize_data=(data-np.mean(data))/np.std(data)  #对数据进行标准化 （数据 - 均值）/（方差）\n",
    "    normalize_data=normalize_data[:,np.newaxis]       #增加数据的维度，使数据维度相同\n",
    "    for i in range(len(normalize_data)-time_step-1):\n",
    "        x=normalize_data[i:i+time_step]\n",
    "        y=normalize_data[i+1:i+time_step+1]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist()) \n",
    "    with tf.variable_scope('lstm',reuse=True):\n",
    "        pred,last= lstm(1)\n",
    "    saver=tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        #参数恢复\n",
    "        saver.restore(sess, './model_save1/modle.ckpt') #第二个参数是保存的地址，可以修改为自己本地的保存地址\n",
    "        #I run the code in windows 10,so use  'model_save1\\\\modle.ckpt'\n",
    "        #if you run it in Linux,please use  'model_save1/modle.ckpt'\n",
    "        \n",
    "        #取训练集最后一行为测试样本。shape = [1,time_step,input_size]\n",
    "        prev_seq=train_x[-1]\n",
    "        predict=[]\n",
    "        #得到之后的100个预测结果\n",
    "        for i in range(n):  #预测100个数值\n",
    "            next_seq=sess.run(pred,feed_dict={X:[prev_seq]})\n",
    "            predict.append(next_seq[-1])\n",
    "            #每次得到最后一个时间步的预测结果，与之前的数据加在一起，形成新的测试数据\n",
    "            prev_seq=np.vstack((prev_seq[1:],next_seq[-1]))\n",
    "        predict = np.squeeze(predict)#减少数据维度\n",
    "        predict = predict*np.std(data)+np.mean(data)#恢复先前价格\n",
    "        return predict;\n",
    "        \n",
    "#fig.add_subplot(1,2,2)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_stock_test_data(\"2012/12/2\",\"2020/1/4\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_acc_true(\"2013/2/2\",\"2016/4/4\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def if_predict_true(start,end,n):   \n",
    "    data = get_stock_data('601318.SS',start,end)\n",
    "    real = data[-n:]\n",
    "    data = data[0:-n]\n",
    "    normalize_data=(data-np.mean(data))/np.std(data)  #对数据进行标准化 （数据 - 均值）/（方差）\n",
    "    normalize_data=normalize_data[:,np.newaxis]       #增加数据的维度，使数据维度相同\n",
    "    for i in range(len(normalize_data)-time_step-1):\n",
    "        x=normalize_data[i:i+time_step]\n",
    "        y=normalize_data[i+1:i+time_step+1]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist()) \n",
    "    with tf.variable_scope('lstm',reuse=True):\n",
    "        pred,last= lstm(1)\n",
    "    saver=tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        #参数恢复\n",
    "        saver.restore(sess, './model_save1/modle.ckpt') #第二个参数是保存的地址，可以修改为自己本地的保存地址\n",
    "        #I run the code in windows 10,so use  'model_save1\\\\modle.ckpt'\n",
    "        #if you run it in Linux,please use  'model_save1/modle.ckpt'\n",
    "\n",
    "        #取训练集最后一行为测试样本。shape = [1,time_step,input_size]\n",
    "        prev_seq=train_x[-1]\n",
    "        predict=[]\n",
    "        #得到之后的100个预测结果\n",
    "        for i in range(n):  #预测100个数值\n",
    "            next_seq=sess.run(pred,feed_dict={X:[prev_seq]})\n",
    "            predict.append(next_seq[-1])\n",
    "            #每次得到最后一个时间步的预测结果，与之前的数据加在一起，形成新的测试数据\n",
    "            prev_seq=np.vstack((prev_seq[1:],next_seq[-1]))\n",
    "        predict = np.squeeze(predict)#减少数据维度\n",
    "        predict = predict*np.std(data)+np.mean(data)#恢复先前价格\n",
    "        if (real[n-1]>data[-1] and predict[n-1]>data[-1]) or (real[n-1]<=data[-1] and predict[n-1]<=data[-1]):\n",
    "            return 1\n",
    "        else:return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_acc_true(start,end,n):   \n",
    "    sum = 0;\n",
    "    data = get_stock_data('601318.SS',start,end)\n",
    "    half = int(len(data)/2)\n",
    "    for times in range(1,half):\n",
    "        t_data = data[times:times+half]\n",
    "        real = t_data[-1:]\n",
    "        t_data = t_data[0:1]\n",
    "        normalize_t_data=(t_data-np.mean(t_data))/np.std(t_data)  #对数据进行标准化 （数据 - 均值）/（方差）\n",
    "        normalize_t_data=normalize_t_data[:,np.newaxis]       #增加数据的维度，使数据维度相同\n",
    "        for i in range(len(normalize_t_data)-time_step-1):\n",
    "            x=normalize_t_data[i:i+time_step]\n",
    "            y=normalize_t_data[i+1:i+time_step+1]\n",
    "            train_x.append(x.tolist())\n",
    "            train_y.append(y.tolist()) \n",
    "        with tf.variable_scope('lstm',reuse=True):\n",
    "            pred,last= lstm(1)\n",
    "        saver=tf.train.Saver(tf.global_variables())\n",
    "        with tf.Session() as sess:\n",
    "            #参数恢复\n",
    "            saver.restore(sess, './model_save1/modle.ckpt') #第二个参数是保存的地址，可以修改为自己本地的保存地址\n",
    "            #I run the code in windows 10,so use  'model_save1\\\\modle.ckpt'\n",
    "            #if you run it in Linux,please use  'model_save1/modle.ckpt'\n",
    "\n",
    "            #取训练集最后一行为测试样本。shape = [1,time_step,input_size]\n",
    "            prev_seq=train_x[-1]\n",
    "            predict=[]\n",
    "            #得到之后的100个预测结果\n",
    "            for i in range(1):  #预测100个数值\n",
    "                next_seq=sess.run(pred,feed_dict={X:[prev_seq]})\n",
    "                predict.append(next_seq[-1])\n",
    "                #每次得到最后一个时间步的预测结果，与之前的数据加在一起，形成新的测试数据\n",
    "                prev_seq=np.vstack((prev_seq[1:],next_seq[-1]))\n",
    "            predict = np.squeeze(predict)#减少数据维度\n",
    "            predict = predict*np.std(t_data)+np.mean(t_data)#恢复先前价格\n",
    "            print(real[0])\n",
    "            print(predict[0]) \n",
    "            print(t_data[-1])           \n",
    "            if real[0]>t_data[-1] and predict[0]>t_data[-1]:\n",
    "                sum = sum+1\n",
    "        print(sum/times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum = 0.00\n",
    "date_s = datetime.datetime.strptime(\"2013/2/2\",'%Y/%m/%d')\n",
    "date_s_t = int(time.mktime(date_s.timetuple()))\n",
    "date_e = datetime.datetime.strptime(\"2016/2/2\",'%Y/%m/%d')\n",
    "date_e_t = int(time.mktime(date_e.timetuple()))\n",
    "for i in range(1,1000):\n",
    "    add_a_day(date_s)\n",
    "    add_a_day(date_e)\n",
    "    sum += if_predict_true(date_s,date_e,8)\n",
    "    print(str(sum/i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-GPU",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
